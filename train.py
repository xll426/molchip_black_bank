# train.py
import os
import random
import csv

import torch
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torch.nn as nn
from black_bank.dataset import TVCornerDataset
from black_bank.model import CornerYOLOOptimized

import math
import cv2
import numpy as np

import torch
import torch.nn.functional as F
import numpy as np


from black_bank.dataset import TVCornerDataset, build_train_transform,build_val_transform
import os
import random
import csv
import argparse

import matplotlib.pyplot as plt
def focal_bce(pred, target, alpha=0.25, gamma=2.0):
    """pred/target shape: (B,1,H,W)"""
    p = torch.sigmoid(pred)
    pt = p * target + (1 - p) * (1 - target)
    w = alpha * target + (1 - alpha) * (1 - target)
    loss = F.binary_cross_entropy_with_logits(pred, target, reduction='none')
    return (w * (1 - pt).pow(gamma) * loss).mean()

def gaussian2D(shape, sigma=1):
    """
    ç”ŸæˆäºŒç»´é«˜æ–¯æ ¸ï¼Œshape ä¸º (height, width)
    """
    sigma = float(sigma) 
    m, n = [(ss - 1.) / 2. for ss in shape]
    y, x = np.ogrid[-m:m+1, -n:n+1]
    h = np.exp(-(x * x + y * y) / (2 * sigma * sigma))
    h[h < np.finfo(h.dtype).eps * h.max()] = 0
    return h

def draw_gaussian(heatmap, center, sigma):
    """
    åœ¨ heatmapï¼ˆnumpy æ•°ç»„ï¼‰ä¸Šç”»ä¸€ä¸ªäºŒç»´é«˜æ–¯ï¼Œ
    center: (x, y) åæ ‡ï¼ˆæ•´æ•°ï¼Œç‰¹å¾å›¾å°ºåº¦ä¸‹ï¼‰
    sigma: é«˜æ–¯æ ‡å‡†å·®
    å‚è€ƒ CenterNet çš„ç›®æ ‡ç”Ÿæˆæ–¹æ³•
    """
    # tmp_size = sigma * 3
    tmp_size = int(3 * float(sigma))  # å¼ºåˆ¶è½¬æ¢æˆ float â†’ int

    x, y = int(center[0]), int(center[1])
    height, width = heatmap.shape[0], heatmap.shape[1]

    # ä¸Šä¸‹å·¦å³è¾¹ç•Œ
    ul = [int(x - tmp_size), int(y - tmp_size)]
    br = [int(x + tmp_size + 1), int(y + tmp_size + 1)]
    
    # é«˜æ–¯æ ¸å°ºå¯¸
    size = 2 * tmp_size + 1
    g = gaussian2D((size, size), sigma=sigma)
    g = torch.tensor(g, dtype=torch.float32, device=heatmap.device)
    
    # è®¡ç®—é«˜æ–¯æ”¾ç½®åœ¨ heatmap çš„åŒºåŸŸ
    g_x_min = max(0, -ul[0])
    g_y_min = max(0, -ul[1])
    g_x_max = min(br[0], width) - ul[0]
    g_y_max = min(br[1], height) - ul[1]
    
    h_x_min = max(0, ul[0])
    h_y_min = max(0, ul[1])
    h_x_max = min(br[0], width)
    h_y_max = min(br[1], height)
    
    if h_x_min >= h_x_max or h_y_min >= h_y_max:
        return heatmap
    # å°†é«˜æ–¯åŠ åˆ° heatmap ä¸Šï¼ˆä½¿ç”¨é€ç‚¹æœ€å¤§å€¼ï¼‰
    heatmap[h_y_min:h_y_max, h_x_min:h_x_max] = torch.max(
        heatmap[h_y_min:h_y_max, h_x_min:h_x_max],
        g[g_y_min:g_y_max, g_x_min:g_x_max]
    )
    return heatmap

def generate_center_target(B, H, W, center_coords, sigma=1):
    """
    æ ¹æ®æ¯ä¸ªæ ·æœ¬çš„ä¸­å¿ƒåæ ‡ (center_coords, shape (B,2) æ ¼å¼, (x,y) in gridåæ ‡)
    ç”Ÿæˆä¸­å¿ƒçƒ­åŠ›å›¾ç›®æ ‡ï¼Œå½¢çŠ¶ (B,1,H,W)ï¼›
    ä½¿ç”¨äºŒç»´é«˜æ–¯ï¼Œå¯¹æ¯ä¸ªæ ·æœ¬åœ¨ heatmap ä¸Šç”»ä¸€ä¸ªé«˜æ–¯ï¼Œ
    é«˜æ–¯å³°å€¼ä¸º 1ï¼Œè¡°å‡ç”± sigma å†³å®šã€‚
    """
    target_heatmaps = torch.zeros((B, 1, H, W), dtype=torch.float32, device=center_coords.device)
    for b in range(B):
        center = center_coords[b]  # (2,)
        target_heatmaps[b,0] = draw_gaussian(target_heatmaps[b,0], center, sigma)
    return target_heatmaps


def generate_center_target_adaptive_sigma(B, H, W, center_coords, sizes, sigma_scale=0.3):
    """
    è‡ªé€‚åº”ç”Ÿæˆä¸­å¿ƒçƒ­åŠ›å›¾

    Args:
        center_coords: (B, 2)ï¼Œæ¯ä¸ªæ ·æœ¬çš„ä¸­å¿ƒåæ ‡ï¼Œå•ä½ä¸ºç‰¹å¾å›¾åæ ‡ (x, y)
        sizes: (B, 2)ï¼Œæ¯ä¸ªç›®æ ‡çš„å®½é«˜ (w, h)ï¼Œå•ä½ä¸ºç‰¹å¾å›¾åæ ‡ï¼ˆæ³¨æ„ç¼©æ”¾ï¼‰
        sigma_scale: æ§åˆ¶é«˜æ–¯å¤§å°çš„ç³»æ•°ï¼Œé€šå¸¸ä¸º 0.3ï½0.5

    Returns:
        target_heatmaps: (B, 1, H, W)
    """
    target_heatmaps = torch.zeros((B, 1, H, W), dtype=torch.float32, device=center_coords.device)
    for b in range(B):
        center = center_coords[b]     # (x, y)
        w, h = sizes[b]               # ç‰¹å¾å›¾å°ºåº¦ä¸‹çš„ç›®æ ‡å®½é«˜
        sigma = sigma_scale * min(w, h)
        sigma = max(sigma, 1.0)       # é¿å…å¤ªå°ï¼Œè‡³å°‘1.0
        target_heatmaps[b, 0] = draw_gaussian(target_heatmaps[b, 0], center, sigma)
    return target_heatmaps





# -------------------------------------------------------------
# _center_and_corner_loss
#   â€¢ pred_pos  : (P,10,H,W)  â€”â€” åªå«æ­£æ ·æœ¬
#   â€¢ target_pos: (P,25)
#   â€¢ idx_pos   : (P,2)  ç›®æ ‡ä¸­å¿ƒ (gx,gy)  tensor-float
# -------------------------------------------------------------
def _center_and_corner_loss(pred_pos, target_pos, idx_pos,
                            sizes, device='cuda'):
    P, _, H, W = pred_pos.shape
    idx_corner = [(0,1),(6,7),(12,13),(18,19)]

    # -------- 1) ç”Ÿæˆä¸­å¿ƒ target heatmap --------
    # center_t = generate_center_target(P, H, W, idx_pos, sigma)   # (P,1,H,W)
    center_t = generate_center_target_adaptive_sigma(P, H, W, idx_pos, sizes)


    # -------- 2) ä¸­å¿ƒ BCE (channel 1) ----------
    # pos_mask = (center_t > 0.1).float()
    center_logits = pred_pos[:,1:2]
    # center_bce = F.binary_cross_entropy_with_logits(
    #     center_logits, center_t, reduction='none')
    # center_loss = (center_bce * pos_mask).sum() / (pos_mask.sum() + 1e-6)
    center_loss = focal_bce(center_logits, center_t, alpha=0.25, gamma=2.0)

    # -------- 3) è§’ç‚¹ åˆ†ç±»+å›å½’ ----------
    neis = [(-1,-1),(0,-1),(1,-1),(-1,0),(0,0),(1,0),(-1,1),(0,1),(1,1)]
    # cls_ch = [2,5,8,11]
    reg_ch = [(2,3),(4,5),(6,7),(8,9)]

    # angle_cls, angle_reg, cnt = 0., 0., 0
    angle_reg, cnt =  0., 0
    gx = idx_pos[:,0].long()
    gy = idx_pos[:,1].long()

    for b in range(P):
        for dx,dy in neis:
            nx = (gx[b]+dx).clamp(0,W-1)
            ny = (gy[b]+dy).clamp(0,H-1)
            cnt += 1
            for k,(ix,iy) in enumerate(idx_corner):
                # åˆ†ç±»
                # logit = pred_pos[b, cls_ch[k], ny, nx]
                # angle_cls += F.binary_cross_entropy_with_logits(
                #     logit.unsqueeze(0), torch.ones(1,device=device), reduction='sum')
                # å›å½’
                t_dx = target_pos[b, ix]*W - nx.float()
                t_dy = target_pos[b, iy]*H - ny.float()
                p_dx = pred_pos[b, reg_ch[k][0], ny, nx]
                p_dy = pred_pos[b, reg_ch[k][1], ny, nx]
                angle_reg += F.smooth_l1_loss(
                    p_dx.unsqueeze(0), t_dx.unsqueeze(0), reduction='sum') + \
                    F.smooth_l1_loss(
                    p_dy.unsqueeze(0), t_dy.unsqueeze(0), reduction='sum')

    # angle_cls_loss = angle_cls / (P*H*W)
    angle_reg_loss = angle_reg / cnt
    return center_loss, angle_reg_loss



def detection_loss_new_center(pred, target, has_target,
                              device='cuda', lambda_reg=1.0, sigma=2.0):
    """
    pred       : (B,14,H,W)  logits
    target     : (B,25)
    has_target : (B,)  1=æ­£æ ·æœ¬, 0=è´Ÿæ ·æœ¬
    """
    B, _, H, W = pred.shape
    exists = has_target.to(pred.dtype)           # (B,)
    sizes = torch.zeros((B, 2), dtype=pred.dtype, device=device)
    # ---------- èƒŒæ™¯ BCE (æ‰€æœ‰æ ·æœ¬éƒ½ç®—) ----------
    bg_target = torch.ones((B,1,H,W), device=device)
    idx_corner = [(0,1),(6,7),(12,13),(18,19)]
    gx = torch.zeros(B, device=device)
    gy = torch.zeros(B, device=device)
    for b in range(B):
        if exists[b] > 0.5:
            xs = [target[b,i] for (i,_) in idx_corner]
            ys = [target[b, j] for (_, j) in idx_corner]
            x_min = min(xs) * W
            x_max = max(xs) * W
            y_min = min(ys) * H
            y_max = max(ys) * H

            w = x_max - x_min
            h = y_max - y_min


            sizes[b] = torch.tensor([w, h])
            gx[b] = sum(xs)/4.0 * W
            gy[b] = sum(ys)/4.0 * H

    neis = [(-1,-1),(0,-1),(1,-1),(-1,0),(0,0),(1,0),(-1,1),(0,1),(1,1)]
    for b in range(B):
        if exists[b] > 0.5:
            for dx,dy in neis:
                x = (gx[b]+dx).clamp(0,W-1).long()
                y = (gy[b]+dy).clamp(0,H-1).long()
                bg_target[b,0,y,x] = 0.

    # bg_loss = F.binary_cross_entropy_with_logits(
    #     pred[:,0:1], bg_target, reduction='mean')
    bg_loss = focal_bce(pred[:,0:1], bg_target)

    # -------------  æ­£æ ·æœ¬åˆ†æ”¯  -------------
    if exists.sum() > 0:
        pos_idx = exists.nonzero(as_tuple=True)[0]         # (P,)
        # æ”¶é›†æ­£æ ·æœ¬ä¸­å¿ƒåæ ‡ (grid å•ä½ float)
        center_pos = torch.stack([gx[pos_idx], gy[pos_idx]], 1) / 1.0
        center_loss,  angle_reg_loss = \
            _center_and_corner_loss(pred[pos_idx], target[pos_idx],
                                    center_pos, sizes, device=device)
    else:
        center_loss = angle_reg_loss = torch.tensor(0., device=device)

    # -------- æ€» loss --------
    # total_loss = bg_loss + 5.0*center_loss + angle_cls_loss + lambda_reg*angle_reg_loss
    # cls_loss_detached = (bg_loss + 5.0*center_loss + angle_cls_loss).detach()
    
    total_loss = bg_loss + 5.0*center_loss + lambda_reg*angle_reg_loss
    cls_loss_detached = (bg_loss + 5.0*center_loss).detach()

    return total_loss, cls_loss_detached, angle_reg_loss.detach()



def visualize_dataloader_samples(dataloader, save_dir='debug_vis', max_batches=1):
    """
    ä» dataloader ä¸­å–è‹¥å¹²ä¸ª batchï¼ˆé»˜è®¤ 1 ä¸ªï¼‰ï¼Œå¯¹æ¯å¼ å›¾ï¼ˆå·²è¢« letterbox ç¼©æ”¾åˆ°640Ã—640ï¼‰çš„
    æ£€æµ‹æ ‡æ³¨ï¼ˆå››è§’ç‚¹+æ–¹å‘ï¼‰åŠåˆ†å‰² mask è¿›è¡Œå¯è§†åŒ–ï¼Œå¹¶ä¿å­˜åˆ°æœ¬åœ°ã€‚

    æ•°æ®æ ¼å¼ï¼š
      images: (B, 3, 640, 640)
      targets: (B, 25)   # å‰24ç»´ä¸º 4 ä¸ªè§’ç‚¹ï¼ˆæ¯è§’6ç»´ï¼‰ï¼Œç¬¬25ç»´ä¸º obj_conf
      segs: (B, 1, 640, 640)  # åˆ†å‰² maskï¼Œå€¼åœ¨ 0ï½1ä¹‹é—´

    å¦‚æœ obj_conf > 0.5ï¼Œåˆ™åœ¨å›¾ä¸Šç”»ï¼š
      - 4ä¸ªè§’ç‚¹ï¼ˆè½¬æ¢æˆç»å¯¹åæ ‡ï¼‰
      - æŒ‰é¡ºæ—¶é’ˆé¡ºåºçš„å¤šè¾¹å½¢ï¼ˆ1->2->3->4->1ï¼‰
      - æ¯ä¸ªè§’ç‚¹å¯¹åº”çš„å‰å‘ã€åå‘å°çº¿æ®µï¼ˆè¡¨ç¤ºæ–¹å‘å‘é‡ï¼‰

    åŒæ—¶ï¼Œå°† seg mask è½¬æ¢ä¸ºä¼ªå½©è‰²çƒ­åŠ›å›¾ï¼Œå¹¶ä¸æ£€æµ‹æ ‡æ³¨å›¾è¿›è¡ŒåŠ æƒå åŠ ï¼Œæœ€åä¿å­˜ç»“æœã€‚
    """
    os.makedirs(save_dir, exist_ok=True)

    with torch.no_grad():
        for batch_idx, (images, targets, segs,flags) in enumerate(dataloader):
            # images: (B,3,640,640); targets: (B,25); segs: (B,1,640,640)
            batch_size = images.size(0)
            for i in range(batch_size):
                # 1. å°†å›¾åƒè½¬æ¢æˆ OpenCV BGR æ ¼å¼
                if flags[i]:
                    img_tensor = images[i]  # (3,640,640)
                    tgt = targets[i]        # (25,)
                    seg = segs[i]           # (1,640,640)
                    
                    img_np = img_tensor.permute(1, 2, 0).cpu().numpy()  # (640,640,3), RGB
                    img_np = (img_np * 255).astype(np.uint8)
                    img_np = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)

                    # 2. åˆ¤æ–­ obj_confï¼ˆæœ€å1ä¸ªæ•°ï¼‰
                    obj_conf = tgt[-1].item()
                    if obj_conf > 0.5:
                        # è§£æå‰24ç»´ï¼šæ¯è§’é¡ºåºï¼š [x, y, cos_forward, sin_forward, cos_backward, sin_backward]
                        corners_abs = []
                        lines_forward = []
                        lines_backward = []
                        for corner_idx in range(4):
                            base = corner_idx * 6
                            x_norm = tgt[base + 0].item()  # [0,1]
                            y_norm = tgt[base + 1].item()
                            cf = tgt[base + 2].item()  # cos forward
                            sf = tgt[base + 3].item()  # sin forward
                            cb = tgt[base + 4].item()  # cos backward
                            sb = tgt[base + 5].item()  # sin backward

                            # è½¬ä¸ºç»å¯¹åƒç´ åæ ‡ï¼ˆ640x640ï¼‰
                            x_abs = x_norm * 640
                            y_abs = y_norm * 640
                            corners_abs.append((x_abs, y_abs))

                            # å®šä¹‰ç»˜åˆ¶çº¿æ®µé•¿åº¦ï¼Œå¦‚30åƒç´ 
                            line_len = 30
                            xF = x_abs + cf * line_len
                            yF = y_abs + sf * line_len
                            xB = x_abs + cb * line_len
                            yB = y_abs + sb * line_len
                            lines_forward.append(((x_abs, y_abs), (xF, yF)))
                            lines_backward.append(((x_abs, y_abs), (xB, yB)))

                        # ç”»å¤šè¾¹å½¢è¾¹ç•Œ
                        poly_pts = np.array(corners_abs, dtype=np.int32).reshape((-1, 1, 2))
                        cv2.polylines(img_np, [poly_pts], isClosed=True, color=(0, 0, 255), thickness=2)
                        # ç»˜åˆ¶æ¯ä¸ªè§’ç‚¹
                        for (cx, cy) in corners_abs:
                            cv2.circle(img_np, (int(cx), int(cy)), radius=3, color=(0, 255, 0), thickness=-1)
                        # ç»˜åˆ¶å‰å‘å’Œåå‘å°çº¿æ®µ
                        for (p1, p2) in lines_forward:
                            cv2.line(img_np,
                                    (int(p1[0]), int(p1[1])),
                                    (int(p2[0]), int(p2[1])),
                                    (0, 255, 255), 2)  # é»„ç»¿è‰²
                        for (p1, p2) in lines_backward:
                            cv2.line(img_np,
                                    (int(p1[0]), int(p1[1])),
                                    (int(p2[0]), int(p2[1])),
                                    (255, 255, 0), 2)  # æµ…è“

                    # 3. åˆ†å‰² mask å¯è§†åŒ–ï¼šseg çš„å½¢çŠ¶ (1,640,640)ï¼Œå–ç¬¬ä¸€ä¸ªé€šé“
                    # è½¬æ¢ä¸º numpy, å½’ä¸€åŒ–åˆ° [0,255]ï¼Œå¹¶ç”¨ä¼ªå½©è‰²æ˜¾ç¤º
                    # seg_np = seg[0].detach().cpu().numpy()  # (640,640), å€¼åœ¨0~1
                    # seg_uint8 = np.clip(seg_np * 255, 0, 255).astype(np.uint8)
                    # seg_color = cv2.applyColorMap(seg_uint8, cv2.COLORMAP_JET)
                    
                    # # 4. å°†åˆ†å‰² heatmap å åŠ åˆ°æ£€æµ‹æ ‡æ³¨å›¾ä¸Šï¼ˆä¾‹å¦‚ 40%å åŠ ï¼‰
                    # alpha = 0.4
                    # overlay = cv2.addWeighted(img_np, 1.0, seg_color, alpha, 0)

                                    # 3. åˆ†å‰² mask å¯è§†åŒ–ï¼šseg çš„å½¢çŠ¶ (1,640,640)ï¼Œå–ç¬¬ä¸€ä¸ªé€šé“
                    seg_np = seg[0].detach().cpu().numpy()  # (640,640), å€¼åœ¨0~1ä¹‹é—´
                    # äºŒå€¼åŒ–: å¤§äºé˜ˆå€¼çš„ä½ç½®ä¸º True
                    threshold = 1
                    mask_bin = seg_np == threshold  # å¸ƒå°”æ•°ç»„

                    # ç”Ÿæˆä¸€ä¸ªä¸åŸå›¾åŒå°ºå¯¸çš„çº¯è‰² overlayï¼Œé¢œè‰²è®¾ä¸ºäº®çº¢è‰² (BGR: (0, 0, 255))
                    color_overlay = np.zeros_like(img_np, dtype=np.uint8)
                    # å°† mask ä¸º True çš„åƒç´ ç‚¹èµ‹äºˆäº®çº¢è‰²
                    color_overlay[mask_bin] = (0, 0, 255)

                    # å åŠ ï¼šåŸå›¾ä¸äº®çº¢ overlay æ··åˆï¼Œalpha å¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼ˆä¾‹å¦‚ 0.5ï¼‰
                    alpha = 0.5
                    overlay = cv2.addWeighted(img_np, 1.0, color_overlay, alpha, 0)

                    
                    # 5. ä¿å­˜ç»“æœï¼ˆæ–‡ä»¶åä¸­åŒæ—¶è®°å½• batch ä¸ç´¢å¼•ï¼‰
                    out_path = os.path.join(save_dir, f"batch{batch_idx}_idx{i}.jpg")
                    cv2.imwrite(out_path, overlay)
                    print(f"Saved visualization: {out_path}")
                    
            if batch_idx+1 >= max_batches:
                break










   

# ============ è®­ç»ƒ/éªŒè¯å¾ªç¯ ============


def collate_fn(batch):
    imgs, tgts, segs, flags = zip(*batch)
    return (torch.stack(imgs),
            torch.stack(tgts),
            torch.stack(segs),
            torch.tensor(flags))              # (B,)  bool / 0-1


def replace_silu_with_relu(module: nn.Module):
    """
    é€’å½’åœ°å°† module åŠå…¶å­æ¨¡å—ä¸­çš„ SiLU æ›¿æ¢ä¸º ReLU(inplace=True)ã€‚
    """
    for name, child in module.named_children():
        if isinstance(child, nn.SiLU):
            setattr(module, name, nn.ReLU(inplace=True))
        else:
            replace_silu_with_relu(child)



# è¯„ä¼°å‡½æ•°ï¼šæ£€æµ‹ä»»åŠ¡ï¼ˆä½¿ç”¨ detection_lossï¼‰
def evaluate_detection(model, loader, device='cpu'):
    model.eval()
    total_loss = 0.0
    count = 0
    with torch.no_grad():
        for imgs, tgts, segs, flags in loader:
            images = imgs.to(device)
            targets = tgts.to(device)
            _, det_out = model(images)
            loss, _, _ = detection_loss_new_center(det_out, targets, flags, device=device)
            total_loss += loss.item()
            count += 1
    return total_loss / (count + 1e-6)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--img_dir', default='/mnt/d/cv_molchip/black_bank/output/image', type=str)
    parser.add_argument('--label_dir', default='/mnt/d/cv_molchip/black_bank/output/label', type=str)
    parser.add_argument('--epochs', default=300, type=int)
    parser.add_argument('--batch_size', default=16, type=int)
    parser.add_argument('--lr', default=1e-4, type=float)
    parser.add_argument('--device', default='cuda', type=str)
    parser.add_argument('--img_w_size', default=640, type=int)
    parser.add_argument('--img_h_size', default=640, type=int)
    parser.add_argument('--resume', default='/mnt/d/cv_molchip/keypoint_detection_16/checkpoints/last_checkpoint.pth', type=str, help='path to resume checkpoint')
    parser.add_argument('--plot_path', default='loss_curve.png', type=str, help='path to save loss plot')
    args = parser.parse_args()

    device = args.device if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")

    # Prepare data loaders
    all_files = [f for f in os.listdir(args.img_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]
    random.shuffle(all_files)
    split_idx = int(0.8 * len(all_files))
    train_files = all_files[:split_idx]
    val_files = all_files[split_idx:]






    kernel_count = os.cpu_count()
    print(kernel_count)
    train_dataset = TVCornerDataset(img_dir=args.img_dir,label_dir=args.label_dir,file_list=train_files,img_w=args.img_w_size,img_h=args.img_h_size,transform=build_train_transform(args.img_w_size,args.img_h_size))
    val_dataset = TVCornerDataset(img_dir=args.img_dir,label_dir=args.label_dir,file_list=val_files,img_w=args.img_w_size,img_h=args.img_h_size,transform=build_val_transform(args.img_w_size,args.img_h_size))
  
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=kernel_count, pin_memory=False, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=kernel_count, pin_memory=False, collate_fn=collate_fn)
    # visualize_dataloader_samples(train_loader, save_dir='debug_vis', max_batches=1)
    # visualize_dataloader_samples(val_loader, save_dir='debug_vis', max_batches=1)
    # raise NotImplementedError


    # Create checkpoint directory
    os.makedirs('checkpoints', exist_ok=True)

    # Initialize model, optimizer, scheduler
    model = CornerYOLOOptimized().to(device)
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr)   
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)

    start_epoch = 0
    best_det_loss = float('inf')

    # Resume from checkpoint if provided
    if args.resume and os.path.isfile(args.resume):
        print(f"Loading checkpoint '{args.resume}'")
        checkpoint = torch.load(args.resume, map_location=device)
        # model.load_state_dict(checkpoint['model_state_dict'])

        # missing, unexpected = model.load_state_dict(checkpoint['model_state_dict'],strict=False)                 # â† å…³é”®
        # print(f"âœ” loaded.  missing={len(missing)}, unexpected={len(unexpected)}")
        # åŠ è½½ checkpoint

        state_dict = checkpoint['model_state_dict']
        model_state = model.state_dict()

        # è¿‡æ»¤æ‰ shape ä¸ä¸€è‡´çš„å±‚
        filtered_dict = {k: v for k, v in state_dict.items() if k in model_state and v.shape == model_state[k].shape}
        print(f"âœ” filtered out {len(state_dict) - len(filtered_dict)} mismatched layers")

        # åŠ è½½åŒ¹é…çš„éƒ¨åˆ†
        model.load_state_dict(filtered_dict, strict=False)



        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        best_det_loss = checkpoint.get('best_det_loss', best_det_loss)
        print(f"Resuming from epoch {start_epoch}, best_det_loss={best_det_loss:.4f}")
  
    epochs_list = []
    train_losses = []
    val_losses = []

    import time  # åŠ åœ¨æ–‡ä»¶å¤´éƒ¨
    best_det_loss =7
    # Training loop
    for epoch in range(start_epoch, args.epochs):
        print(best_det_loss)
        start_time = time.time()  # ğŸ•’ å¼€å§‹æ—¶é—´
        

        model.train()
        total_loss = 0.0
        steps = 0

        for imgs, tgts, segs, flags in train_loader:
            # batch_start = time.time()  # ğŸ•’ æ¯ä¸ª batch å¼€å§‹
            imgs, tgts, flags = imgs.to(device), tgts.to(device), flags.to(device)
            _, preds = model(imgs)
            loss, cls_loss, reg_loss = detection_loss_new_center(preds, tgts, flags, device=device)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            steps += 1
            # batch_end = time.time()  # ğŸ•’ æ¯ä¸ª batch ç»“æŸ
            # print(f"  [Batch {steps}] Loss={loss.item():.4f} | Time={batch_end - batch_start:.2f}s")

        avg_loss = total_loss / max(steps, 1)
        scheduler.step()

        # Validation
        val_loss = evaluate_detection(model, val_loader, device=device)

        # ğŸ•’ ç»“æŸæ—¶é—´å¹¶æ‰“å°
        end_time = time.time()
        elapsed = end_time - start_time
        print(f"[Epoch {epoch+1}] TrainLoss={avg_loss:.4f} | ValLoss={val_loss:.4f} | LR={optimizer.param_groups[0]['lr']:.6f} | Time={elapsed:.2f}s")

        # Record for plotting
        epochs_list.append(epoch + 1)
        train_losses.append(avg_loss)
        val_losses.append(val_loss)

  

        # Save checkpoints
        ckpt = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'best_det_loss': best_det_loss,
        }
        torch.save(ckpt, 'checkpoints/last_checkpoint.pth')
        print(val_loss)
        print(best_det_loss)
        # Save best
        if val_loss < best_det_loss:
            best_det_loss = val_loss
            torch.save(ckpt, 'checkpoints/best_checkpoint.pth')
            print("  ==> New best checkpoint saved!")

    print(f"Training complete. Best validation loss: {best_det_loss:.4f}")



    # Plot and save
    plt.figure()
    plt.plot(epochs_list, train_losses, label='Train Loss')
    plt.plot(epochs_list, val_losses, label='Val Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.savefig(args.plot_path)
    plt.close()


if __name__ == "__main__":
    main()

